{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training process of Machine Learning Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start Jupyter in the environment 'Python 3.11.4 ('env': venv) (~/Desktop/test_project/env/bin/python)'. \n",
      "ImportError: cannot import name 'notebookapp' from 'notebook' (/Users/kirillklimushin/Desktop/test_project/env/lib/python3.11/site-packages/notebook/__init__.py) \n",
      "View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import xgboost as xgb\n",
    "import numpy\n",
    "from calibration import calibrators, plots\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = pandas.read_csv(\"../../data/processed_data/training_set.csv\")\n",
    "validation_set = pandas.read_csv(\"../../data/processed_data/validation_set.csv\")\n",
    "testing_set = pandas.read_csv(\"../../data/processed_data/testing_set.csv\")\n",
    "calibration_set = pandas.read_csv(\"../../data/processed_data/calibration_set.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting Calibration set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = calibration_set.drop(columns=['bad_client']), calibration_set['bad_client']\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.3, random_state=100\n",
    ")\n",
    "\n",
    "train_calibration_set = pandas.concat([x_train, y_train], axis=1)\n",
    "test_calibration_set = pandas.concat([x_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, make_scorer, log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_validate "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = training_set.drop(columns=['bad_client'])\n",
    "y_train = training_set['bad_client']\n",
    "\n",
    "loss_function = make_scorer(log_loss, greater_is_better=False)\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    estimator=model,\n",
    "    X=x_train,\n",
    "    y=y_train,\n",
    "    n_jobs=-1,\n",
    "    cv=StratifiedKFold(n_splits=10),\n",
    "    scoring=loss_function\n",
    ")\n",
    "# Checking model training loss\n",
    "cv_score = numpy.mean(cv_results['test_score'])\n",
    "print('loss: %s' % cv_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine Tuning the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "}\n",
    "\n",
    "X_validation = validation_set.drop(columns=['bad_client'], inplace=False)\n",
    "Y_validation = validation_set['bad_client']\n",
    "\n",
    "loss_function = make_scorer(log_loss, greater_is_better=False)\n",
    "tuned_model = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=hyperparams,\n",
    "    scoring=loss_function,\n",
    "    n_jobs=-1,\n",
    "    cv=StratifiedKFold(n_splits=5)\n",
    ")\n",
    "# Fitting model\n",
    "tuned_model.fit(X_validation, Y_validation)\n",
    "chosen_model = tuned_model['cv_results']['best_estimator']\n",
    "loss = numpy.mean(tuned_model['test_score'])\n",
    "print('hyperparameter validation loss: %s' % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_calibration = train_calibration_set.drop(columns=['bad_client'])\n",
    "y_train_calibration = chosen_model.predict(x_train_calibration)\n",
    "y_train_calibration_proba = chosen_model.predict_proba(x_train_calibration)[:, 1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking Calibration quality of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots.calibration_plot(\n",
    "    y_true=train_calibration_set['bad_client'],\n",
    "    y_pred=y_train_calibration,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Training Calibration Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_train_dataset = calibrators.CalibrationDataset(\n",
    "    decision_scores=y_train_calibration_proba,\n",
    "    true_classes=train_calibration_set['bad_client']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Calibration Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "platt_scaler = calibrators.PlattScaling()\n",
    "\n",
    "# training calibration algorithm\n",
    "platt_scaler.train(\n",
    "    train_dataset=calibration_train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Testing Calibration Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting decision scores from given testing calibration data\n",
    "x_test_calibration = test_calibration_set.drop(columns=['bad_client'])\n",
    "predicted_test_calibration_scores = chosen_model.predict_proba(x_test_calibration)[:, 1].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating calibration dataset\n",
    "\n",
    "calibration_test_dataset = calibrators.CalibrationDataset(\n",
    "    decision_scores=predicted_test_calibration_scores,\n",
    "    true_classes=test_calibration_set['bad_client']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating Calibration Algorihtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_classes = calibration_test_dataset.true_classes\n",
    "predicted_probs = platt_scaler.get_calibrated_prob(\n",
    "    decision_scores=calibration_test_dataset.decision_scores\n",
    ")\n",
    "predicted_classes = (predicted_probs >= 0.5).astype(numpy.int_)\n",
    "\n",
    "plots.calibration_plot(\n",
    "    y_true=true_classes,\n",
    "    y_pred=predicted_classes\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing ML model on a Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = testing_set.drop(columns=['bad_client'])\n",
    "Y_test = testing_set['bad_client']\n",
    "\n",
    "def eval_f1_weighted(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Standard F1 Score metric with weighted average\n",
    "    \"\"\"\n",
    "    return f1_score(\n",
    "        y_true, y_pred,\n",
    "        average='weighted'\n",
    "    )\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    estimator=chosen_model,\n",
    "    X=X_test,\n",
    "    Y=Y_test,\n",
    "    cv=StratifiedKFold(n_splits=5),\n",
    "    scoring=eval_f1_weighted,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print('F1 Score: %s' % numpy.mean(cv_results['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(chosen_model, open('../models/classifier.pkl', mode='wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving Calibration Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(platt_scaler, open(\"../calibrators/platt_calibrator.pkl\", mode='wb'))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c9d868ccd008ea04b9c5a002fb183b0949d8c1b424380ded90a0df80fb5d714"
  },
  "kernelspec": {
   "display_name": "Python 3.11.4 ('credit_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
