{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas, numpy\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "applications = pandas.read_csv(\"./application_record.csv\")\n",
    "credit_records = pandas.read_csv(\"./credit_record.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at each dataset individually \n",
    "applications.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets' see how the credit record looks like \n",
    "# 1 - due less 30 days\n",
    "# 2 - due 30 - 60 days \n",
    "# 3 - due 60 - 90 days\n",
    "# 4 - due 90 - 150 days \n",
    "# 5 - due over 150 days \n",
    "\n",
    "credit_records.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relabling Credit Records Dataset\n",
    "\n",
    "CREDIT_MAPPING = {\n",
    "    \"ID\": \"client_id\",\n",
    "    \"STATUS\": \"due_status\",\n",
    "    \"MONTHS_BALANCE\": \"balance_month\"\n",
    "}\n",
    "credit_records.rename(columns=CREDIT_MAPPING, inplace=True)\n",
    "\n",
    "# Relabling Applications Records Dataset\n",
    "APPLICATION_MAPPING = {\n",
    "    \"ID\": \"client_id\",\n",
    "    \"CODE_GENDER\": \"gender\",\n",
    "    \"FLAG_OWN_CAR\": \"has_car\",\n",
    "    \"FLAG_OWN_REALTY\": \"has_realty\",\n",
    "    \"CNT_CHILDREN\": \"total_children\",\n",
    "    \"AMT_INCOME_TOTAL\": \"annual_income\",\n",
    "    \"NAME_INCOME_TYPE\": \"income_category\",\n",
    "    \"NAME_EDUCATION_TYPE\": \"education_category\",\n",
    "    \"NAME_FAMILY_STATUS\": \"family_status\",\n",
    "    \"NAME_HOUSING_TYPE\": \"living_place\",\n",
    "    \"DAYS_BIRTH\": \"birthday\",\n",
    "    \"FLAG_MOBIL\": \"has_mobile_phone\",\n",
    "    \"FLAG_PHONE\": \"has_phone\",\n",
    "    \"FLAG_WORK_PHONE\": \"has_work_phone\",\n",
    "    \"FLAG_EMAIL\": \"has_email\",\n",
    "    \"CNT_FAM_MEMBERS\": \"family_size\",\n",
    "    \"OCCUPATION_TYPE\": \"job\",\n",
    "    'DAYS_EMPLOYED': 'days_employed',\n",
    "}\n",
    "\n",
    "applications.rename(columns=APPLICATION_MAPPING, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Editing Dataset Labels for better understanding and easier usage \n",
    "applications.info()\n",
    "applications.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many nan values we have in credit records dataset\n",
    "credit_records.info() \n",
    "credit_records.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many unique application records we have\n",
    "applications['client_id'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many unique credit record we have\n",
    "credit_records[\"client_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's see how many intersections to we have \n",
    "applications.merge(credit_records, on=\"client_id\")[\"client_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating credit window for each individual customer \n",
    "\n",
    "grouped = credit_records.groupby('client_id')\n",
    "customer_credit = pandas.DataFrame()\n",
    "\n",
    "customer_credit['open_month'] = grouped['balance_month'].min() # smallest value of MONTHS_BALANCE, is the month when loan was granted\n",
    "customer_credit['end_month'] = grouped['balance_month'].max() # biggest value of MONTHS_BALANCE, might be observe over or canceling account\n",
    "\n",
    "customer_credit[\"credit_window\"] = customer_credit[\"end_month\"] - customer_credit[\"open_month\"] + 1\n",
    "customer_credit.reset_index(inplace=True)\n",
    "\n",
    "customer_credit = customer_credit[['client_id', 'credit_window']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining with main credit table and finding credit difference\n",
    "print(credit_records)\n",
    "\n",
    "DUE_DATATYPES = {\n",
    "    \"0\": 0,\n",
    "    \"1\": 0,\n",
    "    \"2\": 1,\n",
    "    \"3\": 1,\n",
    "    \"4\": 1,\n",
    "    \"5\": 1,\n",
    "    \"X\": 0,\n",
    "    \"C\": 0\n",
    "}\n",
    "\n",
    "def convert_due_status(status: str):\n",
    "    \"\"\"\n",
    "    Transforms digit due statuses to meaningful names\n",
    "    everything, that is higher than 1 marks as 'overdue', which indicates, \n",
    "    that client had a huge pay out delay and might be not reliable\n",
    "    \"\"\"\n",
    "    if not isinstance(status, str): return 0\n",
    "\n",
    "    if status.isdigit():\n",
    "        if int(status) > 1: \n",
    "            return 1 \n",
    "    return 0\n",
    "\n",
    "# Mapping values according to specified standards\n",
    "\n",
    "credit_records[\"due_status\"] = credit_records['due_status'].map(lambda item: DUE_DATATYPES[str(item)])\n",
    "\n",
    "# Overdue metrics per client\n",
    "overdue_per_client = credit_records[[\"client_id\", \"due_status\"]].groupby(\n",
    "by=[\"client_id\"])['due_status'].sum().to_frame('overdue_frequency').reset_index()\n",
    "\n",
    "customer_credit = customer_credit.merge(overdue_per_client, on='client_id', how='inner')\n",
    "\n",
    "customer_credit[\"bad_client\"] = overdue_per_client[\"overdue_frequency\"].apply(\n",
    "func=lambda item: True if item > 0 else False)\n",
    "\n",
    "customer_credit = customer_credit[[\"client_id\", \"credit_window\", \"bad_client\"]]\n",
    "\n",
    "def set_datatypes(dataset: pandas.DataFrame):\n",
    "    \"\"\"\n",
    "    Optimizing datatypes for dataset compression\n",
    "    \"\"\"\n",
    "    dataset[\"client_id\"] = dataset[\"client_id\"].astype(numpy.int64)\n",
    "    dataset[\"credit_window\"] = dataset[\"credit_window\"].astype(numpy.int16)\n",
    "    return dataset\n",
    "\n",
    "completed_feature_dataset = set_datatypes(customer_credit)\n",
    "\n",
    "# After Optimizing our dataset, let's check it's characteristics \n",
    "\n",
    "completed_feature_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging with main application records using left join (we want to keep customer's which does not have any )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how many clients are good (can be potentially considered by the banks) and bad clients\n",
    "\n",
    "sns.countplot(data=completed_feature_dataset, x=\"bad_client\")\n",
    "plt.xlabel(\"Proportion of good and bad customers. Bad - 1; Good - 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate majority class to get more information about it \n",
    "\n",
    "data = completed_feature_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's try different techniques for managing imbalanced datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling Techniques for dealing with imbalanced datasets\n",
    "import typing, logging\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import precision_score\n",
    "import pandas \n",
    "import constants \n",
    "\n",
    "Logger = logging.getLogger(__name__)\n",
    "\n",
    "def rose_over_sampling(\n",
    "    X_train: typing.Union[pandas.DataFrame, pandas.Series],\n",
    "    Y_train: pandas.Series\n",
    "):\n",
    "    if not len(X_train) or not len(Y_train): return None, None\n",
    "    try:\n",
    "        sampler  = RandomOverSampler(random_state=1)\n",
    "        X_resampled, Y_resampled = sampler.fit_resample(X=X_train, y=Y_train)\n",
    "        return X_resampled, Y_resampled \n",
    "    except(TypeError, ValueError) as sampling_exception:\n",
    "        Logger.debug(\"Failed to perform ROSE Over Sampling Technique, Exception Arised. [%s]\"\n",
    "        % sampling_exception)\n",
    "        return None, None\n",
    "\n",
    "def rose_under_sampling(X_train: pandas.DataFrame, Y_train: pandas.Series):\n",
    "    if not len(X_train) or not len(Y_train): return None, None\n",
    "    try:\n",
    "        sampler = RandomUnderSampler(random_state=1)\n",
    "        X_resampled, Y_resampled = sampler.fit_resample(X=X_train, y=Y_train)\n",
    "        return X_resampled, Y_resampled \n",
    "    except(TypeError, ValueError) as sampling_exception:\n",
    "        Logger.debug(\"Failed to perform ROSE Under Sampling Technique, Exception Arised. [%s]\"\n",
    "        % sampling_exception)\n",
    "        return None, None\n",
    "    \n",
    "\n",
    "def smote_sampling(\n",
    "    X_train: typing.Union[pandas.DataFrame, pandas.Series], \n",
    "    Y_train: typing.Union[pandas.DataFrame, pandas.Series]):\n",
    "\n",
    "    if not len(X_train) or not len(Y_train): return None, None \n",
    "    try:\n",
    "        smote_tech = SMOTE(random_state=1, k_neighbors=constants.K_SMOTE_NEIGHBORS)\n",
    "        X_resampled, Y_resampled = smote_tech.fit_resample(X_train, Y_train)\n",
    "        return X_resampled, Y_resampled\n",
    "    except(TypeError, ValueError) as train_exception:\n",
    "        Logger.debug(\"Failed to balance data using SMOTE Technique, exception raised. [%s]\" % train_exception)\n",
    "        return None, None \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's split data and try out SMOTE method on our data\n",
    "\n",
    "X_data = data.drop(columns=[\"bad_client\"])\n",
    "Y_data = data[\"bad_client\"]\n",
    "\n",
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "# Splitting data on training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_data, Y_data, random_state=1)\n",
    "\n",
    "# Transforming each dataset individually \n",
    "\n",
    "SM_XR_sampled, SM_YR_sampled = smote_sampling(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize new data\n",
    "SM_YR_sampled.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing Feature Encodings to the dataset\n",
    "sns.countplot(SM_YR_sampled.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging with application records feature dataset\n",
    "credit_feature_dataset = pandas.concat([SM_XR_sampled, SM_YR_sampled], axis=1)\n",
    "\n",
    "feature_dataset = credit_feature_dataset.merge(applications, on=\"client_id\")\n",
    "\n",
    "# Dropping duplicated rows\n",
    "feature_dataset.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values before encoding\n",
    "\n",
    "feature_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Mode Imputation for Nominal Categorical Feature 'Job'\n",
    "MISSING_JOB = \"missing_job\"\n",
    "feature_dataset['job'].fillna(MISSING_JOB, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Imputation for Education and Family Status categories \n",
    "feature_dataset['education_category'] = feature_dataset['education_category'].apply(func=lambda item: item.lower()) \n",
    "feature_dataset['family_status'] = feature_dataset['family_status'].apply(func=lambda item: item.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Engineering features for application records \n",
    "\n",
    "import math \n",
    "\n",
    "feature_dataset['employed']  = feature_dataset['days_employed'].apply(func=lambda day: False if day > 0 else True)\n",
    "feature_dataset['age'] = feature_dataset['birthday'].apply(func=lambda day: math.floor(abs(day) / 365))\n",
    "\n",
    "feature_dataset.drop(columns=[\"days_employed\", \"birthday\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the imputation status \n",
    "feature_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Binary Features\n",
    "\n",
    "def encode_binary_features(binary_dataset: pandas.DataFrame):\n",
    "    \"\"\"\n",
    "    Function encodes binary features to corresponding boolean values \n",
    "    Yes - True\n",
    "    No - False\n",
    "    \n",
    "    It is implied to convert binary features to appropriate format, because \n",
    "    some models does not do well with categorical variables and requires feature tranformation\n",
    "    \n",
    "    Args:\n",
    "        dataset: input dataset, which contains binary features\n",
    "    \"\"\"\n",
    "    try:\n",
    "        binary_dataset.apply(func=(lambda item: True if item == \"Y\" else False), axis=1)\n",
    "        return None\n",
    "    except():\n",
    "        pass\n",
    "    \n",
    "def encode_gender_feature(gender_feature: pandas.Series) -> pandas.DataFrame:\n",
    "    \"\"\"\n",
    "    Encodes Gender Feature using One-Hot Encoding\n",
    "    Args:\n",
    "        gender_feature - pandas.Series object, which contains \n",
    "        about the client's gender\n",
    "    \"\"\"\n",
    "    gender_features = pandas.get_dummies(gender_feature).rename(\n",
    "    columns={'F': 'Female', 'M': 'Male'})\n",
    "    return gender_features\n",
    "\n",
    "\n",
    "binary_features = [\"has_car\", \"has_mobile_phone\", \n",
    "\"has_phone\", \"has_email\", \"has_work_phone\", \"has_realty\"]\n",
    "\n",
    "encode_binary_features(feature_dataset[binary_features])\n",
    "gender_features = encode_gender_feature(feature_dataset['gender'])\n",
    "\n",
    "\n",
    "# Dropping gender column \n",
    "feature_dataset.drop(columns=['gender'], axis=1)\n",
    "\n",
    "# combining with encoded gender frame \n",
    "feature_dataset = pandas.concat([feature_dataset, gender_features], axis=1)\n",
    "\n",
    "feature_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Multi class categorical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing Education Category \n",
    "\n",
    "feature_dataset['education_category'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 5 unique groups in the Education Category Dataset Feature \n",
    "# Let's review each of the them \n",
    "feature_dataset['education_category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Education Data has certain airarphy, in that case, we can consider applying Ordinal Encoder \n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder \n",
    "\n",
    "def scale_hierarhical_feature(data_series: pandas.Series, categories: typing.List[str]):\n",
    "    \"\"\"\n",
    "    Function scales feature using Ordinal Encodind\n",
    "    \n",
    "    Notes:\n",
    "        1. Feature should have logical hierarchy, otherwise it would not make sense\n",
    "        Common example is school grades: 2, 3, 4, 5, which denotes corresponding feedback about work \n",
    "        in hierarchical order\n",
    "        \n",
    "    Args:\n",
    "        1. data_series: pandas.Series object, that should be encoded\n",
    "        2. categories: hierarchically ordered list of categories (from top to bottom)\n",
    "        \n",
    "    Returns:\n",
    "        pandas.Series object with ordinal-encoded values\n",
    "    \"\"\"\n",
    "    if data_series.isna().sum() > 0:\n",
    "        raise ValueError(\"Series contains null values\")\n",
    "        \n",
    "    scaler = OrdinalEncoder(categories=categories) \n",
    "    scaled_data = scaler.fit_transform(data_series)\n",
    "    return scaled_data\n",
    "\n",
    "# Updating Education Category Labels \n",
    "\n",
    "feature_dataset['education_category'] = feature_dataset['education_category'].map({\n",
    "    'secondary / secondary special': 'special secondary',\n",
    "    'incomplete higher': 'incomplete higher education',\n",
    "    'academic degree': 'academic degree',\n",
    "    'higher education': 'higher education',\n",
    "    'lower secondary': 'lower secondary'\n",
    "})\n",
    "\n",
    "feature_dataset[\"education_category\"] = scale_hierarhical_feature(\n",
    "    data_series=feature_dataset[['education_category']],\n",
    "    categories=[\n",
    "        [\n",
    "        \"academic degree\",\n",
    "        \"higher education\",\n",
    "        \"incomplete higher education\",\n",
    "        \"special secondary\",\n",
    "        \"lower secondary\"\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "feature_dataset['education_category']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Family Status categorical feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing Family status unique categories \n",
    "\n",
    "feature_dataset['education_category'].unique() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating which approach would be the most applicable in our case scenario "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_family_status(feature_series: pandas.Series):\n",
    "    if feature_series.isna().sum() > 0: \n",
    "        raise TypeError(\"Feature Series contains null values\")\n",
    "    try:\n",
    "        return pandas.get_dummies(feature_series)\n",
    "    except(TypeError, ValueError, AttributeError) as exc:\n",
    "        Logger.error(exc)\n",
    "        return numpy.fill(len(feature_series), 1), \"unknown value\", dtype='string')\n",
    "\n",
    "        \n",
    "encoded_family_status = encode_family_status(feature_dataset['family_status'])\n",
    "\n",
    "\n",
    "# concatenating tables together \n",
    "feature_dataset = pandas.concat([feature_dataset, encoded_family_status], axis=1)\n",
    "\n",
    "# dropping family status column, as we no longer need it\n",
    "feature_dataset.drop(columns=[\"family_status\"], inplace=True)\n",
    "\n",
    "feature_dataset['']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standartizing Numeric Features \n",
    "\n",
    "numeric_set = feature_dataset.select_dtypes(include='number').columns\n",
    "numeric_set = numeric_set[1:].tolist()\n",
    "\n",
    "numeric_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "\n",
    "scaler = StandardScaler() \n",
    "scaled_data = pandas.DataFrame(scaler.fit_transform(feature_dataset[numeric_set]), \n",
    "columns=numeric_set)\n",
    "\n",
    "scaled_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging back to the main feature table \n",
    "\n",
    "feature_dataset[numeric_set] = scaled_data\n",
    "\n",
    "feature_dataset.reset_index(inplace=True)\n",
    "\n",
    "feature_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Multi-class categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "953cdab6cce91cb4ff4265ecd1f71841d0f3ed6f837521e1b030c6273aab7b8e"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
